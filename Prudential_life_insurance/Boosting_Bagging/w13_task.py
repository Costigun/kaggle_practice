# -*- coding: utf-8 -*-
"""w13_Task.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tt8Kq0eECbUMlrgUyRweCD2IO1K2mLy0

Загрузите данные, приведите их к числовым, заполните пропуски, нормализуйте данные и оптимизируйте память.

Разделите выборку на обучающую/проверочную в соотношении 80/20.

Постройте ансамбль решающих деревьев, используя градиентный бустинг (GradientBoostingClassifier). Используйте перекрестную проверку, чтобы найти наилучшие параметры ансамбля, или используйте параметры от случайного леса: max_depth=17, max_features=27, min_samples_leaf=20, n_estimators=76.

Проведите предсказание и проверьте качество через каппа-метрику.

Данные: video.ittensive.com/machine-learning/prudential/train.csv.gz

Вопросы к этому заданию
Какая точность градиентного бустинга по каппа метрике с точностью до десятых? Например, 0.0
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import cohen_kappa_score,confusion_matrix,plot_confusion_matrix,make_scorer
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV, cross_val_score
from xgboost import XGBClassifier
from sklearn import preprocessing

# !pip install xgboost

data = pd.read_csv("https://video.ittensive.com/machine-learning/prudential/train.csv.gz")

data['Product_Info_2_1'] = data['Product_Info_2'].str.slice(0,1)
data['Product_Info_2_2'] = data['Product_Info_2'].str.slice(1,2)
data.drop('Product_Info_2',axis=1,inplace=True)
for letter in data['Product_Info_2_1'].unique():
    data['Product_Info_2_1' + letter] = data['Product_Info_2_1'].isin([letter]).astype('int8')
data.drop('Product_Info_2_1',axis=1,inplace=True)

"""Заполнение пропусков"""

data.fillna(-1,inplace=True)
data['Response'] = data['Response'] - 1

"""Столбцы для рассчета"""

columns_groups = ["Insurance_History", "InsurеdInfo", "Medical_Keyword",
                  "Family_Hist", "Medical_History", "Product_Info"]
columns = ["Wt", "Ht", "Ins_Age", "BMI"]
for cg in columns_groups:
    columns.extend(data.columns[data.columns.str.startswith(cg)])

"""Нормализация данных"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
data_transformed = pd.DataFrame(scaler.fit_transform(data[columns]))

columns = data_transformed.columns

data_transformed['Response'] = data['Response']

"""Разделение данных на обучающую и тестовую"""

from sklearn.model_selection import train_test_split

data_train,data_test = train_test_split(data_transformed,test_size=0.2)

x = data_train.drop('Response',axis=1)
y = data_train['Response']

"""Применим градиентный бустинг с поиском оптимальных параметров"""

model = GradientBoostingClassifier()
params = {
    'n_estimators':range(75,77),
    'max_depth':range(15,17),
    'max_features':range(25,27),
    'min_samples_leaf':range(18,20)
}

grid_model = GridSearchCV(model,params,cv=5,n_jobs=2,verbose=True,scoring=make_scorer(cohen_kappa_score))

grid_model.fit(x,y)

cohen_kappa_score(data['target'],data['Response'],weights='quadratic')