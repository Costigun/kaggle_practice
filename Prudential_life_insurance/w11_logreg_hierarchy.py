# -*- coding: utf-8 -*-
"""w11_LogReg_Hierarchy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GQGlQibsvHyTPEczCmzK-G7k8kQBHzAO

## Постановка задачи
Загрузим данные, приведем их к числовым, заполним пропуски, нормализуем данные и оптимизируем память.

Разделим выборку на обучающую/проверочную в соотношении 80/20.

Построим 4 модели логистической регрессии: для 8, 6 и остальных классов, для 2, 5 и остальных, для 1, 7 и остальных, и для 4 и 3 - по убыванию частоты значения. Будем использовать перекрестную проверку при принятии решения об оптимальном наборе столбцов.

Проведем предсказание и проверим качество через каппа-метрику.

Данные:
* https://video.ittensive.com/machine-learning/prudential/train.csv.gz

Соревнование: https://www.kaggle.com/c/prudential-life-insurance-assessment/

© ITtensive, 2020
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import cohen_kappa_score, confusion_matrix, make_scorer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn import preprocessing

data = pd.read_csv("https://video.ittensive.com/machine-learning/prudential/train.csv.gz")
data = data.iloc[:1000]
print (data.info())

data["Product_Info_2_1"] = data["Product_Info_2"].str.slice(0, 1)
data["Product_Info_2_2"] = pd.to_numeric(data["Product_Info_2"].str.slice(1, 2))
data.drop(labels=["Product_Info_2"], axis=1, inplace=True)
for l in data["Product_Info_2_1"].unique():
    data["Product_Info_2_1" + l] = data["Product_Info_2_1"].isin([l]).astype("int8")
data.drop(labels=["Product_Info_2_1"], axis=1, inplace=True)
data.fillna(value=-1, inplace=True)

def reduce_mem_usage (df):
    start_mem = df.memory_usage().sum() / 1024**2    
    for col in df.columns:
        col_type = df[col].dtypes
        if str(col_type)[:5] == "float":
            c_min = df[col].min()
            c_max = df[col].max()
            if c_min > np.finfo("f2").min and c_max < np.finfo("f2").max:
                df[col] = df[col].astype(np.float16)
            elif c_min > np.finfo("f4").min and c_max < np.finfo("f4").max:
                df[col] = df[col].astype(np.float32)
            else:
                df[col] = df[col].astype(np.float64)
        elif str(col_type)[:3] == "int":
            c_min = df[col].min()
            c_max = df[col].max()
            if c_min > np.iinfo("i1").min and c_max < np.iinfo("i1").max:
                df[col] = df[col].astype(np.int8)
            elif c_min > np.iinfo("i2").min and c_max < np.iinfo("i2").max:
                df[col] = df[col].astype(np.int16)
            elif c_min > np.iinfo("i4").min and c_max < np.iinfo("i4").max:
                df[col] = df[col].astype(np.int32)
            elif c_min > np.iinfo("i8").min and c_max < np.iinfo("i8").max:
                df[col] = df[col].astype(np.int64)
        else:
            df[col] = df[col].astype("category")
    end_mem = df.memory_usage().sum() / 1024**2
    print('Потребление памяти меньше на', round(start_mem - end_mem, 2), 'Мб (минус', round(100 * (start_mem - end_mem) / start_mem, 1), '%)')
    return df

data = reduce_mem_usage(data)
print (data.info())

columns_groups = ["Insurance_History", "InsurеdInfo", "Medical_Keyword",
                  "Family_Hist", "Medical_History", "Product_Info"]
columns = ["Wt", "Ht", "Ins_Age", "BMI"]
for cg in columns_groups:
    columns.extend(data.columns[data.columns.str.startswith(cg)])
print (columns)

scaler = preprocessing.StandardScaler()
data_transformed = pd.DataFrame(scaler.fit_transform(data[columns]))

columns_transofrmed = data_transformed.columns

data_transformed['Response'] = data['Response']

data_train,data_test = train_test_split(data_transformed,test_size=0.2)



def regression_model(columns,df):
    y = df['Response']
    x = df.drop('Response',axis=1)
    model = LogisticRegression(max_iter=1000)
    model.fit(x,y)
    return model

def logistic_regression(columns,df_train):
    model = regression_model(columns,df_train)
    logr_grid = GridSearchCV(model,{},cv=5,n_jobs=2,
                            scoring=make_scorer(cohen_kappa_score))
    x = df_train.drop('Response',axis=1)
    y = df_train['Response']
    logr_grid.fit(x,y)
    return logr_grid.best_score_

"""### Оптимальный набор столбцов
Для каждого уровня иерархии это будет свой набор столбцов в исходных данных.

### Перекрестная проверка
Разбиваем обучающую выборку еще на k (часто 5) частей, на каждой части данных обучаем модель. Затем проверяем 1-ю, 2-ю, 3-ю, 4-ю части на 5; 1-ю, 2-ю, 3-ю, 5-ю части на 4 и т.д.

В итоге обучение пройдет весь набор данных, и каждая часть набора будет проверена на всех оставшихся (перекрестным образом).
"""

def find_opt_col(data_train):
    kappa_score_opt = 0
    columns_opt = []
    for col in columns_transofrmed:
        kappa_score = logistic_regression([col],data_train)
        if kappa_score > kappa_score_opt:
            columns_opt = [col]
            kappa_score = kappa_score_opt
    for col in columns_transofrmed:
        if col not in columns_opt:
            columns_opt.append(col)
            kappa_score = logistic_regression(columns_opt,data_train)
            if kappa_score < kappa_score_opt:
                columns_opt.pop()
            else:
                kappa_score_opt = kappa_score
    print(col)
    return columns_opt,kappa_score_opt

responses = [[6,8],[2,5],[1,7],[3,4]]
logr_models = [{}] * len(responses)
data_train_current = data_train.copy()

i = 0

for response in responses:
    m_train = data_train_current.copy()
    if response != [3,4]:
        m_train['Response'] = m_train['Response'].apply(lambda x:0 if x not in response else x)
    columns_opt,kappa_score_opt = find_opt_col(m_train)
    print(i,kappa_score_opt,columns_opt)
    logr_models[i] = {
        "model":regression_model(columns_opt,m_train),
        "columns":columns_opt
    }
    if response != [3,4]:
        data_train_current = data_train_current[~data_train_current['Response'].isin(response)]
    i += 1

def logr_hierarchy(x):
  for response in range(0,len(responses)):
    if x['target' + str(response)] > 0:
      x['target'] = x['target' + str(response)]
      break;
  return x

for response in range(0,len(responses)):
  model = logr_models[response]['model']
  columns_opt = logr_models[response]['columns']
  x = pd.DataFrame(data_test,columns=columns_opt)
  data_test['target' + str(response)] = model.predict(x)

data_test = data_test.apply(logr_hierarchy,axis=1,result_type='expand')

data_test.head()

cohen_kappa_score(data_test['target'],data_test['Response'],weights='quadratic')

